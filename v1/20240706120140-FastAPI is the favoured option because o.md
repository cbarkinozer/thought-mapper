---
date: 2024-07-06T12:01:40
platform: X
topics:
  - Large Language Models
source: https://twitter.com/user/status/1809558331255624167
---
# FastAPI is the favoured option because of its async capabilities. If you're serving a model, consider Ray Serve. I prefer to use FastAPI with vLLM for inference, which serves the model with Ray Serve in the background.

## Topics
- [[Large Language Models]]

## Tags
#LargeLanguageModels